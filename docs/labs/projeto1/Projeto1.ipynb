{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9/RumKTIWhc2ZTZDnWKRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aavarela/SPBD_Labs/blob/main/docs/labs/projeto1/Projeto1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Java setup (required for pyspark)"
      ],
      "metadata": {
        "id": "dWI0DDeOeHjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smT6Cr02eCXK"
      },
      "outputs": [],
      "source": [
        "#@title Java Setup (needed for pyspark)\n",
        "!apt-get install -y openjdk-17-jre 2>/dev/null > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download 1% sample"
      ],
      "metadata": {
        "id": "G3_FqOFLeY9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download 1% sample\n",
        "!wget -q -O taxi_rides_1pc.csv.gz https://www.dropbox.com/scl/fi/v8ei5laqcalrx30z3lsty/taxi_rides_1pc.csv.gz?rlkey=q1lq7l56c4j97h9kymsdroau5&st=iurdwnwj&dl=0"
      ],
      "metadata": {
        "id": "lfxJ2fDdee6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inspect dataset schema"
      ],
      "metadata": {
        "id": "62lA2jWuejSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Schema\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('taxis').getOrCreate()\n",
        "\n",
        "try :\n",
        "    data = spark.read.csv('taxi_rides_1pc.csv.gz', sep =',', header=True, inferSchema=True)\n",
        "\n",
        "    data.printSchema()\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "id": "WSYhI6SteoJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Register the provided helper functions (latlon_to_grid and inBounds) as user defined functions (UDF)"
      ],
      "metadata": {
        "id": "RX6_6MGWesin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, IntegerType, BooleanType, DoubleType\n",
        "\n",
        "# Longitude and latitude from the upper left corner of the grid\n",
        "MIN_LON = -74.916578\n",
        "MAX_LAT = 41.47718278\n",
        "\n",
        "# Longitude and latitude that correspond to a shift in 500 meters\n",
        "LON_DELTA = 0.005986\n",
        "LAT_DELTA = 0.004491556\n",
        "\n",
        "def latlon_to_grid(lat, lon):\n",
        "    return ((int)((MAX_LAT - lat)/LAT_DELTA), (int)((lon - MIN_LON)/LON_DELTA))\n",
        "\n",
        "def inBounds( cell ):\n",
        "    return cell[0] > 0 and cell[0] < 300 and cell[1] > 0 and cell[1] < 300\n",
        "\n",
        "# Register latlon_to_grid as a UDF\n",
        "latlon_to_grid_udf = udf(latlon_to_grid, ArrayType(IntegerType()))\n",
        "\n",
        "# Register inBounds as a UDF\n",
        "inBounds_udf = udf(inBounds, BooleanType())"
      ],
      "metadata": {
        "id": "yjH1QvNhe7Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clean the data by removing null and invalid coordinates"
      ],
      "metadata": {
        "id": "kZH95txRfJen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, isnan, when\n",
        "\n",
        "# Filter out rows with null or invalid coordinate values before applying UDFs\n",
        "# Check for both None and NaN if the schema is double\n",
        "cleaned_data = data.filter(\n",
        "    col(\"pickup_latitude\").isNotNull() & ~isnan(col(\"pickup_latitude\")) &\n",
        "    col(\"pickup_longitude\").isNotNull() & ~isnan(col(\"pickup_longitude\")) &\n",
        "    col(\"dropoff_latitude\").isNotNull() & ~isnan(col(\"dropoff_latitude\")) &\n",
        "    col(\"dropoff_longitude\").isNotNull() & ~isnan(col(\"dropoff_longitude\"))\n",
        ")"
      ],
      "metadata": {
        "id": "SIGckFmVfRYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert the coordinates using the helper functions"
      ],
      "metadata": {
        "id": "zhDVUkk0fWOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply latlon_to_grid_udf to pickup and dropoff coordinates\n",
        "data_with_grid_coords = cleaned_data \\\n",
        "    .withColumn(\"pickup_grid_coords\", latlon_to_grid_udf(col(\"pickup_latitude\"), col(\"pickup_longitude\"))) \\\n",
        "    .withColumn(\"dropoff_grid_coords\", latlon_to_grid_udf(col(\"dropoff_latitude\"), col(\"dropoff_longitude\")))\n",
        "\n",
        "# Extract x and y coordinates\n",
        "data_with_grid_coords = data_with_grid_coords \\\n",
        "    .withColumn(\"pickup_grid_x\", col(\"pickup_grid_coords\").getItem(0)) \\\n",
        "    .withColumn(\"pickup_grid_y\", col(\"pickup_grid_coords\").getItem(1)) \\\n",
        "    .withColumn(\"dropoff_grid_x\", col(\"dropoff_grid_coords\").getItem(0)) \\\n",
        "    .withColumn(\"dropoff_grid_y\", col(\"dropoff_grid_coords\").getItem(1))"
      ],
      "metadata": {
        "id": "WT6jYC6yfeuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Filter the data by removing entries that are outside the grid"
      ],
      "metadata": {
        "id": "XCnyijQIfh6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter DataFrame to include only trips within the 300x300 grid for both pickup and dropoff\n",
        "filtered_data = data_with_grid_coords.filter(\n",
        "    inBounds_udf(col(\"pickup_grid_coords\")) & inBounds_udf(col(\"dropoff_grid_coords\"))\n",
        ")"
      ],
      "metadata": {
        "id": "DFZaAXIWfrsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate the trips profit"
      ],
      "metadata": {
        "id": "Kzz9xnXlfvVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate trip_profit\n",
        "final_data = filtered_data.withColumn(\"trip_profit\", col(\"fare_amount\") + col(\"tip_amount\"))"
      ],
      "metadata": {
        "id": "qGiVgE9Df4AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Display data reduction and final data schema"
      ],
      "metadata": {
        "id": "Gcd9xcnBf7kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original DataFrame count:\", data.count())\n",
        "print(\"Cleaned DataFrame count (after removing null coords):\", cleaned_data.count())\n",
        "print(\"DataFrame with grid coordinates, filtered and profit count:\", final_data.count())\n",
        "final_data.printSchema()"
      ],
      "metadata": {
        "id": "fML6owM4gWUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}