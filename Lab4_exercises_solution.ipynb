{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aavarela/SPBD_Labs/blob/main/Lab4_exercises_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsPM1Z4HH7M"
      },
      "source": [
        "# Python Spark Exercises\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download \"Os Maias\"\n",
        "!wget -q -O os_maias.txt https://www.dropbox.com/s/n24v0z7y79np319/os_maias.txt?dl=0\n",
        "!pip install --quiet unidecode"
      ],
      "metadata": {
        "id": "GEn0_HxQHDlx",
        "outputId": "6ac07844-53ef-4753-f7a8-d8eac7459f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Word Frequency Exercises\n",
        "\n",
        "1.1) Create a [Spark Core](https://spark.apache.org/docs/latest/api/python/) program that counts the number of occurrences of each word in the novel \"Os Maias\":\n",
        "\n",
        "a) No sorting required, words and frequency can appear in any order;\n",
        "\n",
        "b) Sorted by word, in reverse alphabetical order;\n",
        "\n",
        "c) Sorted by frequency (the words with higher occurrence first).\n",
        "\n",
        "Note that the sorting should be performed as a transformation (i.e. it should produce an RDD)..."
      ],
      "metadata": {
        "id": "f7oXYnylGyko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title WordFrequency Spark Core 1.1a)\n",
        "import pyspark, string\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from unidecode import unidecode\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"WordFrequency1.1a\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "      .map( lambda line: line.strip() ) \\\n",
        "      .map( lambda line: unidecode(line).lower() ) \\\n",
        "      .map( lambda line: line.translate(str.maketrans('', '', string.punctuation+'«»')) ) \\\n",
        "\n",
        "  words = lines.flatMap( lambda line: line.split() ) \\\n",
        "          .map( lambda word: (word, 1)) \\\n",
        "          .reduceByKey( lambda a, b: a+b)\n",
        "\n",
        "\n",
        "  for w,f in words.take(20):\n",
        "      print(\"{}\\t{}\".format(w,f))\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()\n"
      ],
      "metadata": {
        "id": "qMFaHhpWHxkB",
        "outputId": "e257da3f-14ba-44ce-c12f-78387d2bee80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eca\t1\n",
            "capitulo\t20\n",
            "casa\t327\n",
            "no\t1569\n",
            "outono\t21\n",
            "na\t1320\n",
            "da\t2237\n",
            "rua\t163\n",
            "paula\t2\n",
            "o\t7581\n",
            "bairro\t21\n",
            "das\t480\n",
            "verdes\t14\n",
            "apesar\t40\n",
            "nome\t112\n",
            "sombrio\t24\n",
            "casarao\t8\n",
            "paredes\t19\n",
            "severas\t3\n",
            "um\t3191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title WordFrequency Spark Core 1.1b)\n",
        "import pyspark, string\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from unidecode import unidecode\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"WordFrequency1.1b\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "      .map( lambda line: line.strip() ) \\\n",
        "      .map( lambda line: unidecode(line).lower() ) \\\n",
        "      .map( lambda line: line.translate(str.maketrans('', '', string.punctuation+'«»')) ) \\\n",
        "\n",
        "  words = lines.flatMap( lambda line: line.split() ) \\\n",
        "          .map( lambda word: (word, 1)) \\\n",
        "          .reduceByKey( lambda a, b: a+b)\n",
        "\n",
        "  sorted_words = words.sortByKey(ascending=False)\n",
        "\n",
        "  for w,f in sorted_words.take(20):\n",
        "      print(\"{}\\t{}\".format(w,f))\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "jocH9TZyUbMF",
        "outputId": "9281a3e9-e17e-4c8a-9d51-098eb1da6412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zurzira\t1\n",
            "zumzum\t1\n",
            "zumbir\t1\n",
            "zuavos\t1\n",
            "zombarias\t1\n",
            "zola\t3\n",
            "zinco\t3\n",
            "zeloso\t1\n",
            "zelosamente\t1\n",
            "zelo\t1\n",
            "zelava\t1\n",
            "zelandia\t1\n",
            "zeferino\t1\n",
            "zas\t7\n",
            "zangues\t2\n",
            "zangouse\t2\n",
            "zangava\t1\n",
            "zangase\t1\n",
            "zangarse\t1\n",
            "zangar\t3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title WordFrequency Spark Core 1.1c)\n",
        "import pyspark, string\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from unidecode import unidecode\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"WordFrequency1.1c\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "      .map( lambda line: line.strip() ) \\\n",
        "      .map( lambda line: unidecode(line).lower() ) \\\n",
        "      .map( lambda line: line.translate(str.maketrans('', '', string.punctuation+'«»')) ) \\\n",
        "\n",
        "  words = lines.flatMap( lambda line: line.split() ) \\\n",
        "          .map( lambda word: (word, 1)) \\\n",
        "          .reduceByKey( lambda a, b: a+b)\n",
        "\n",
        "  sorted_words = words.sortBy(lambda t : t[1], ascending=False)\n",
        "\n",
        "  for w,f in sorted_words.take(20):\n",
        "      print(\"{}\\t{}\".format(w,f))\n",
        "\n",
        "  sc.stop()\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  sc.stop()"
      ],
      "metadata": {
        "id": "xVqHaqC4AsJp",
        "outputId": "1b06072f-b40b-485c-872b-38a382ea3020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de\t8464\n",
            "a\t8136\n",
            "o\t7581\n",
            "e\t7526\n",
            "que\t5309\n",
            "um\t3191\n",
            "com\t2872\n",
            "do\t2592\n",
            "uma\t2298\n",
            "da\t2237\n",
            "nao\t2169\n",
            "os\t1888\n",
            "para\t1822\n",
            "carlos\t1798\n",
            "as\t1694\n",
            "em\t1626\n",
            "no\t1569\n",
            "se\t1534\n",
            "ao\t1487\n",
            "na\t1320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2) Create a Spark program that the top 10 most used words in \"Os Maias\" novel.\n",
        "\n",
        "You should try to avoid sorting or finding the top-10 as actions. Your top-10 most used words should be a RDD at the end of the computation. Check *zipWithIndex* in [pyspark RDD](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD) documentation"
      ],
      "metadata": {
        "id": "UkI4QSo8Ua35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.2) Top10Words Version 1\n",
        "import pyspark, string\n",
        "from pyspark.sql import SparkSession\n",
        "from unidecode import unidecode\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Top10Words 1.2) v1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "      .map( lambda line: line.strip() ) \\\n",
        "      .map( lambda line: unidecode(line).lower() ) \\\n",
        "      .map( lambda line: line.translate(str.maketrans('', '', string.punctuation+'«»')) ) \\\n",
        "\n",
        "\n",
        "  top10_words_partitions = lines.flatMap( lambda line: line.split() ) \\\n",
        "          .map( lambda word: (word, 1)) \\\n",
        "          .reduceByKey( lambda a, b: a + b) \\\n",
        "          .mapPartitions( lambda partition : sorted(partition, key=lambda kv: kv[1], reverse=True)[0:10])\n",
        "\n",
        "  top10_words = top10_words_partitions.sortBy(lambda x: x[1], ascending = False) \\\n",
        "                .zipWithIndex() \\\n",
        "                .filter( lambda ranked: ranked[1] < 10) \\\n",
        "                .map( lambda ranked: ranked[0])\n",
        "\n",
        "  print(\"Partitions: Top-10\")\n",
        "  for x in top10_words_partitions.glom().collect():\n",
        "      print(\"{}\".format(x))\n",
        "\n",
        "  print(\"Top-10 Most frequent words:\")\n",
        "  for x in top10_words.collect():\n",
        "      print(\"{}\".format(x))\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ],
      "metadata": {
        "id": "8Z6fWfIiBNmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e6779b-5154-420e-8a69-b04bb1d5b587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions: Top-10\n",
            "[('o', 7581), ('um', 3191), ('uma', 2298), ('da', 2237), ('para', 1822), ('as', 1694), ('no', 1569), ('ao', 1487), ('na', 1320), ('por', 1094)]\n",
            "[('de', 8464), ('a', 8136), ('e', 7526), ('que', 5309), ('com', 2872), ('do', 2592), ('nao', 2169), ('os', 1888), ('carlos', 1798), ('em', 1626)]\n",
            "Top-10 Most frequent words:\n",
            "('de', 8464)\n",
            "('a', 8136)\n",
            "('o', 7581)\n",
            "('e', 7526)\n",
            "('que', 5309)\n",
            "('um', 3191)\n",
            "('com', 2872)\n",
            "('do', 2592)\n",
            "('uma', 2298)\n",
            "('da', 2237)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.2) Top10Words Version 2\n",
        "import pyspark, string\n",
        "from pyspark.sql import SparkSession\n",
        "from unidecode import unidecode\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Top10Words 1.2) v2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try:\n",
        "  lines = sc.textFile('os_maias.txt') \\\n",
        "      .map( lambda line: line.strip() ) \\\n",
        "      .map( lambda line: unidecode(line).lower() ) \\\n",
        "      .map( lambda line: line.translate(str.maketrans('', '', string.punctuation+'«»')) ) \\\n",
        "\n",
        "  top10_words_partitions = lines.flatMap( lambda line: line.split() ) \\\n",
        "          .map( lambda word: (word, 1)) \\\n",
        "          .reduceByKey( lambda a, b: a+b) \\\n",
        "          .glom() \\\n",
        "          .map( lambda partition : sorted(partition, key=lambda kv: kv[1], reverse=True)[0:10]) \\\n",
        "          .flatMap( lambda _ : _ )\n",
        "\n",
        "  top10_words = top10_words_partitions.sortBy(lambda x: x[1], ascending = False) \\\n",
        "                .zipWithIndex() \\\n",
        "                .filter( lambda ranked: ranked[1] < 10) \\\n",
        "                .map( lambda ranked: ranked[0])\n",
        "\n",
        "  print(\"Partitions: Top-10\")\n",
        "  for x in top10_words_partitions.glom().collect():\n",
        "      print(\"{}\".format(x))\n",
        "\n",
        "  print(\"Top-10 Most frequent words:\")\n",
        "  for x in top10_words.collect():\n",
        "      print(\"{}\".format(x))\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "1WpigxXdBQvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871f93ce-78f3-4fa2-8076-3e80b84b9168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions: Top-10\n",
            "[('o', 7581), ('um', 3191), ('uma', 2298), ('da', 2237), ('para', 1822), ('as', 1694), ('no', 1569), ('ao', 1487), ('na', 1320), ('por', 1094)]\n",
            "[('de', 8464), ('a', 8136), ('e', 7526), ('que', 5309), ('com', 2872), ('do', 2592), ('nao', 2169), ('os', 1888), ('carlos', 1798), ('em', 1626)]\n",
            "Top-10 Most frequent words:\n",
            "('de', 8464)\n",
            "('a', 8136)\n",
            "('o', 7581)\n",
            "('e', 7526)\n",
            "('que', 5309)\n",
            "('um', 3191)\n",
            "('com', 2872)\n",
            "('do', 2592)\n",
            "('uma', 2298)\n",
            "('da', 2237)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}